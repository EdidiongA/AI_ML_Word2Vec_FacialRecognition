{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gensim | pip install --upgrade gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anHLF_5qo9fh",
        "outputId": "a2bca388-c940-455d-ffb7-0dbb8b836e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (4.3.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: FuzzyTM>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (2.0.5)\n",
            "Requirement already satisfied: pyfume in /usr/local/lib/python3.8/dist-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from FuzzyTM>=0.4.0->gensim) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7.1)\n",
            "Requirement already satisfied: fst-pso in /usr/local/lib/python3.8/dist-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
            "Requirement already satisfied: simpful in /usr/local/lib/python3.8/dist-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->FuzzyTM>=0.4.0->gensim) (1.15.0)\n",
            "Requirement already satisfied: miniful in /usr/local/lib/python3.8/dist-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.25.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (4.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1168EEGtlest"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import gensim.models\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "#newmodel = gensim.models.KeyedVectors.load_word2vec_format(<path to reducedvector.bin>, binary=True)\n",
        "\n",
        "#mypath = 'crop_part1'\n",
        "#onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "#len = len(onlyfiles)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Task Set #1: Here you will use distributional vectors trained using Googleâ€™s deep learning Word2vec system.**"
      ],
      "metadata": {
        "id": "FP_MPLMkf5Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1: \n",
        "We will use the target words - man and woman. Use the pre-trained word2vec model to rank the following 15 words from the most similar to the least similar to each target word. For each word-target word pair, provide the similarity score. Provide your results in table format.\n",
        "wife\n",
        "husband\n",
        "child\n",
        "queen\n",
        "king\n",
        "man\n",
        "woman\n",
        "birth\n",
        "doctor\n",
        "nurse\n",
        "teacher\n",
        "professor\n",
        "engineer\n",
        "scientist\n",
        "president"
      ],
      "metadata": {
        "id": "7GhbAydEHeAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the line below loads the pre-trained Word2vec model\n",
        "newmodel = gensim.models.KeyedVectors.load_word2vec_format('reducedvector.bin', binary=True)"
      ],
      "metadata": {
        "id": "Yn6dcM42mTah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target words\n",
        "target_words = ['man', 'woman']\n",
        "\n",
        "# Define the list of words to rank\n",
        "word_list = ['wife', 'husband', 'child', 'queen', 'king', 'man', 'woman', 'birth', 'doctor', 'nurse', 'teacher', 'professor', 'engineer', 'scientist', 'president']\n",
        "\n",
        "# Print the table header\n",
        "print('{:15s} {:15s} {:15s}'.format('Word', 'Similar to Man', 'Similar to Woman'))\n",
        "\n",
        "# Loop through each word in the word list\n",
        "for word in word_list:\n",
        "    # Calculate the similarity score for the word and each target word\n",
        "    man_sim = newmodel.similarity(word, target_words[0])\n",
        "    woman_sim = newmodel.similarity(word, target_words[1])\n",
        "    \n",
        "    # Print the word and its similarity scores\n",
        "    print('{:15s} {:.4f} {:15s} {:.4f}'.format(word, man_sim, '', woman_sim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVU_dlu4AFHi",
        "outputId": "b9ec4ced-9601-42a0-d9b3-8414adaf160a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word            Similar to Man  Similar to Woman\n",
            "wife            0.2835                 0.3007\n",
            "husband         0.2341                 0.4496\n",
            "child           0.3334                 0.5898\n",
            "queen           0.1104                 0.2286\n",
            "king            0.2645                 0.1225\n",
            "man             1.0000                 0.5877\n",
            "woman           0.5877                 1.0000\n",
            "birth           0.1234                 0.4203\n",
            "doctor          0.2892                 0.1961\n",
            "nurse           0.1535                 0.2544\n",
            "teacher         0.0987                 0.2041\n",
            "professor       0.1076                 0.1052\n",
            "engineer        0.0874                 0.0443\n",
            "scientist       0.1123                 0.1373\n",
            "president       0.0946                 0.0846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2: \n",
        "The Bigger Analogy Test Set (BATS) Word analogy task has been one of the standard benchmarks\n",
        "for word embeddings since 2013 (https://vecto.space/projects/BATS/ ). A) Select any file from the\n",
        "downloaded dataset (BATS_3.0.zip). For each row in your selected file, choose a target word from the\n",
        "row and provide the measure of similarity between your target word and the other words on the row\n",
        "(Remember to document the file used). B) Think of three words that identify membership in one of the\n",
        "protected classes (choose only one class): race, color, religion, or national origin. For each row in your\n",
        "selected BATS_3.0 file, compute the similarity between your target word and each of your three words.\n",
        "Indicate when there are noticeable differences in the similarity scores based on membership in the\n",
        "protected class. Provide your results in table format."
      ],
      "metadata": {
        "id": "P0ueTifPNIEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#country_capital = 'E01 [country - capital].txt'\n",
        "#data = pd.read_csv(country_capital, sep=\"\\t\", header=None)\n",
        "#data.columns = [\"worda\", \"wordb\"]\n",
        "\n",
        "\n",
        "#data['Similarity'] = data.apply(lambda row: newmodel.similarity(row.worda, row.wordb), axis=1)\n",
        "\n",
        "#data.to_csv(r'country_capital_similarity.csv', index=None, header=True)\n",
        "#def compute_similarity_df_of_target(target, df):\n",
        "#  df[target] = df.apply(lambda row: newmodel.similarity(target, row.wordb), axis=1)\n",
        "#  return df\n",
        "\n",
        "##target_words = ['hindu', 'muslim', 'christian']\n",
        "#target_words = ['niger', 'mali', 'italy']\n",
        "\n",
        "#for word in target_words:\n",
        "#  compute_similarity_df_of_target(word, data)"
      ],
      "metadata": {
        "id": "NAWHx3H9NO6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "#import gensim\n",
        "import os\n",
        "\n",
        "## set the path to the downloaded dataset\n",
        "#dataset_path = \"path/to/BATS_3.0\"\n",
        "\n",
        "# load the pre-trained word2vec model\n",
        "#model_path = \"path/to/pre-trained/word2vec/model\"\n",
        "#model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('reducedvector.bin', binary=True)\n",
        "\n",
        "\n",
        "# select the E01 [country - capital].txt file\n",
        "filename = \"E01 [country - capital].txt\"\n",
        "\n",
        "# read the selected file line by line\n",
        "with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "        # split the line into target and other words\n",
        "        words = line.strip().split()\n",
        "        target_word = random.choice(words)\n",
        "        other_words = [word for word in words if word != target_word]\n",
        "\n",
        "        # calculate similarity for each other word\n",
        "        similarities = {}\n",
        "        for word in other_words:\n",
        "            try:\n",
        "                similarity = model.similarity(target_word, word)\n",
        "                similarities[word] = similarity\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "        # print the similarities\n",
        "        print(\"Target word: \", target_word)\n",
        "        for word, similarity in similarities.items():\n",
        "            print(word, \": \", similarity)\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch_DvidTgNVZ",
        "outputId": "dbc05cac-4eb6-4bdd-97db-1f55cb8a010e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target word:  nigeria\n",
            "\n",
            "Target word:  amman\n",
            "jordan :  0.32452255\n",
            "\n",
            "Target word:  turkey\n",
            "ankara :  0.2951077\n",
            "\n",
            "Target word:  greece\n",
            "athens :  0.51070476\n",
            "\n",
            "Target word:  baghdad\n",
            "iraq :  0.5305512\n",
            "\n",
            "Target word:  thailand\n",
            "bangkok :  0.46890336\n",
            "\n",
            "Target word:  china\n",
            "beijing :  0.56142706\n",
            "\n",
            "Target word:  lebanon\n",
            "beirut :  0.5844345\n",
            "\n",
            "Target word:  belgrade\n",
            "serbia :  0.60130286\n",
            "\n",
            "Target word:  berlin\n",
            "germany :  0.55362153\n",
            "\n",
            "Target word:  switzerland\n",
            "bern :  0.4729466\n",
            "\n",
            "Target word:  belgium\n",
            "brussels :  0.5625071\n",
            "\n",
            "Target word:  romania\n",
            "bucharest :  0.46989092\n",
            "\n",
            "Target word:  hungary\n",
            "budapest :  0.51703984\n",
            "\n",
            "Target word:  cairo\n",
            "egypt :  0.48935854\n",
            "\n",
            "Target word:  canberra\n",
            "australia :  0.34367883\n",
            "\n",
            "Target word:  guinea\n",
            "conakry :  0.5251478\n",
            "\n",
            "Target word:  denmark\n",
            "copenhagen :  0.44661847\n",
            "\n",
            "Target word:  syria\n",
            "damascus :  0.604804\n",
            "\n",
            "Target word:  dhaka\n",
            "bangladesh :  0.45323506\n",
            "\n",
            "Target word:  dublin\n",
            "ireland :  0.43728414\n",
            "\n",
            "Target word:  hanoi\n",
            "vietnam :  0.33919048\n",
            "\n",
            "Target word:  cuba\n",
            "havana :  0.4849839\n",
            "\n",
            "Target word:  helsinki\n",
            "finland :  0.50370324\n",
            "\n",
            "Target word:  pakistan\n",
            "islamabad :  0.57465935\n",
            "\n",
            "Target word:  indonesia\n",
            "jakarta :  0.4288151\n",
            "\n",
            "Target word:  kabul\n",
            "afghanistan :  0.58106303\n",
            "\n",
            "Target word:  kiev\n",
            "ukraine :  0.4809033\n",
            "\n",
            "Target word:  kingston\n",
            "jamaica :  0.41654664\n",
            "\n",
            "Target word:  lima\n",
            "peru :  0.5814075\n",
            "\n",
            "Target word:  portugal\n",
            "lisbon :  0.42609632\n",
            "\n",
            "Target word:  london\n",
            "\n",
            "Target word:  madrid\n",
            "spain :  0.38974234\n",
            "\n",
            "Target word:  manila\n",
            "philippines :  0.41501483\n",
            "\n",
            "Target word:  moscow\n",
            "russia :  0.4628907\n",
            "\n",
            "Target word:  kenya\n",
            "nairobi :  0.5891436\n",
            "\n",
            "Target word:  norway\n",
            "oslo :  0.4161061\n",
            "\n",
            "Target word:  ottawa\n",
            "canada :  0.5074189\n",
            "\n",
            "Target word:  france\n",
            "paris :  0.48489687\n",
            "\n",
            "Target word:  rome\n",
            "italy :  0.44624\n",
            "\n",
            "Target word:  chile\n",
            "santiago :  0.4505333\n",
            "\n",
            "Target word:  bulgaria\n",
            "sofia :  0.30064997\n",
            "\n",
            "Target word:  sweden\n",
            "stockholm :  0.58504677\n",
            "\n",
            "Target word:  taiwan\n",
            "taipei :  0.65781975\n",
            "\n",
            "Target word:  tbilisi\n",
            "georgia :  0.48339292\n",
            "\n",
            "Target word:  tehran\n",
            "iran :  0.44066894\n",
            "\n",
            "Target word:  tokyo\n",
            "japan :  0.47895756\n",
            "\n",
            "Target word:  austria\n",
            "vienna :  0.5364096\n",
            "\n",
            "Target word:  poland\n",
            "warsaw :  0.559183\n",
            "\n",
            "Target word:  croatia\n",
            "zagreb :  0.4985986\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the E01 [country - capital].txt file\n",
        "with open('E02 [country - language].txt', 'r') as f:\n",
        "    data = f.readlines()\n",
        "\n",
        "# Define the target words\n",
        "targets = ['brazil', 'egypt', 'cuba']  #['niger', 'mali', 'italy']\n",
        "\n",
        "# Print table headers\n",
        "print('{:<20}{:<20}{:<20}{:<20}'.format('Row', 'Target', 'Similarity to Target', 'Similarity to Targets'))\n",
        "\n",
        "# Iterate over each row in the file\n",
        "for i, row in enumerate(data):\n",
        "    words = row.strip().split()\n",
        "    target = np.random.choice(words)  # choose a random target word\n",
        "    similarities = []  # to store similarities to target and target words\n",
        "    for word in words:\n",
        "      try:\n",
        "        similarity = newmodel.similarity(target, word)\n",
        "        similarities.append(similarity)\n",
        "      except KeyError:\n",
        "        continue\n",
        "    # compute similarities to target words\n",
        "    target_similarities = [newmodel.similarity(target, t) for t in targets]\n",
        "    # print row data\n",
        "    print('{:<20}{:<20}{:<20}{:<20}'.format(i+1, target, max(similarities), ', '.join(map(str, target_similarities))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "v_n7nkkqRB-7",
        "outputId": "7f42b761-3731-4bef-e191-7b1e54997660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row                 Target              Similarity to TargetSimilarity to Targets\n",
            "1                   catalan             1.0                 0.33535448, 0.015112223, 0.1944874\n",
            "2                   spanish             1.0                 0.31933144, 0.118534714, 0.3720445\n",
            "3                   australia           1.0                 0.38127396, 0.13002697, 0.25139156\n",
            "4                   austria             1.0                 0.28986394, 0.24120441, 0.23149465\n",
            "5                   english             1.0                 0.08928749, -0.054947644, -0.026521971\n",
            "6                   bangladesh          0.9999999403953552  0.42855862, 0.2665325, 0.38061264\n",
            "7                   barbados            1.0                 0.49911553, 0.09804604, 0.48576283\n",
            "8                   english             1.0                 0.08928749, -0.054947644, -0.026521971\n",
            "9                   spanish             1.0                 0.31933144, 0.118534714, 0.3720445\n",
            "10                  brazil              0.9999999403953552  0.99999994, 0.17430171, 0.54311204\n",
            "11                  khmer               0.9999999403953552  0.2083131, 0.22462426, 0.29922965\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6814a6664752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# compute similarities to target words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtarget_similarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnewmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# print row data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:<20}{:<20}{:<20}{:<20}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_similarities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-6814a6664752>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# compute similarities to target words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtarget_similarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnewmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# print row data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:<20}{:<20}{:<20}{:<20}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_similarities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'english/french' not present\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "wCkWVFsHioK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "#import zipfile\n",
        "from gensim.models import KeyedVectors\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Load the pre-trained Word2Vec model\n",
        "#model_path = 'reducedvectors.bin'\n",
        "#model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('reducedvector.bin', binary=True)\n",
        "\n",
        "## Select the E01 [country - capital].txt file from the downloaded dataset BATS_3.0.zip\n",
        "#zip_path = 'BATS_3.0.zip'\n",
        "#with zipfile.ZipFile(zip_path) as z:\n",
        "#    with z.open('BATS_3.0/E01 [country - capital].txt') as f:\n",
        "#        lines = f.read().decode('utf-8').splitlines()\n",
        "\n",
        "with open('E01 [country - capital].txt') as f:\n",
        "        #lines = f.read().decode('utf-8').splitlines()\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "# Initialize the results table\n",
        "table = [['Row', 'Target Word', 'Similarity to Other Words', 'Similarity to Niger', 'Similarity to Mali', 'Similarity to Italy']]\n",
        "\n",
        "# Loop through each row in the selected file\n",
        "for i, line in enumerate(lines):\n",
        "    try:\n",
        "        # Choose a random target word from the row\n",
        "        words = line.split()\n",
        "        target_word = random.choice(words)\n",
        "        print(\"The target word is: \", target_word)\n",
        "\n",
        "        # Compute the similarity between the target word and the other words on the row\n",
        "        similarities = [model.similarity(target_word, word) for word in words]\n",
        "\n",
        "        # Compute the similarity between the target word and the three specified words\n",
        "        niger_sim = model.similarity(target_word, 'niger')\n",
        "        mali_sim = model.similarity(target_word, 'mali')\n",
        "        italy_sim = model.similarity(target_word, 'italy')\n",
        "\n",
        "        # Add the results to the table\n",
        "        row = [i+1, target_word, similarities, niger_sim, mali_sim, italy_sim]\n",
        "        table.append(row)\n",
        "\n",
        "    except KeyError:\n",
        "        # Skip the row if a key error is generated\n",
        "        continue\n",
        "\n",
        "# Print the results table\n",
        "print(tabulate(table, headers='firstrow'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7CQYLCMmWXn",
        "outputId": "d5d440dd-1f76-4221-e188-051a3f4434d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The target word is:  abuja\n",
            "The target word is:  amman\n",
            "The target word is:  turkey\n",
            "The target word is:  greece\n",
            "The target word is:  baghdad\n",
            "The target word is:  thailand\n",
            "The target word is:  beijing\n",
            "The target word is:  lebanon\n",
            "The target word is:  belgrade\n",
            "The target word is:  germany\n",
            "The target word is:  switzerland\n",
            "The target word is:  brussels\n",
            "The target word is:  bucharest\n",
            "The target word is:  budapest\n",
            "The target word is:  cairo\n",
            "The target word is:  australia\n",
            "The target word is:  guinea\n",
            "The target word is:  copenhagen\n",
            "The target word is:  damascus\n",
            "The target word is:  bangladesh\n",
            "The target word is:  dublin\n",
            "The target word is:  vietnam\n",
            "The target word is:  havana\n",
            "The target word is:  helsinki\n",
            "The target word is:  islamabad\n",
            "The target word is:  jakarta\n",
            "The target word is:  kabul\n",
            "The target word is:  ukraine\n",
            "The target word is:  kingston\n",
            "The target word is:  peru\n",
            "The target word is:  lisbon\n",
            "The target word is:  england/uk/britain\n",
            "The target word is:  madrid\n",
            "The target word is:  philippines\n",
            "The target word is:  moscow\n",
            "The target word is:  nairobi\n",
            "The target word is:  norway\n",
            "The target word is:  ottawa\n",
            "The target word is:  france\n",
            "The target word is:  italy\n",
            "The target word is:  santiago\n",
            "The target word is:  sofia\n",
            "The target word is:  stockholm\n",
            "The target word is:  taipei\n",
            "The target word is:  tbilisi\n",
            "The target word is:  iran\n",
            "The target word is:  tokyo\n",
            "The target word is:  austria\n",
            "The target word is:  poland\n",
            "The target word is:  zagreb\n",
            "  Row  Target Word    Similarity to Other Words      Similarity to Niger    Similarity to Mali    Similarity to Italy\n",
            "-----  -------------  ---------------------------  ---------------------  --------------------  ---------------------\n",
            "    2  amman          [0.99999994, 0.32452255]                 0.376343              0.348145               0.224705\n",
            "    3  turkey         [0.2951077, 1.0]                         0.475746              0.473512               0.601875\n",
            "    4  greece         [0.51070476, 1.0]                        0.331451              0.44029                0.590924\n",
            "    5  baghdad        [1.0, 0.5305512]                         0.206435              0.267437               0.237193\n",
            "    6  thailand       [0.46890336, 0.99999994]                 0.545814              0.50752                0.388154\n",
            "    7  beijing        [1.0, 0.56142706]                        0.315615              0.272403               0.119315\n",
            "    8  lebanon        [0.5844345, 1.0]                         0.384251              0.384651               0.275971\n",
            "    9  belgrade       [1.0, 0.60130286]                        0.376045              0.397636               0.416546\n",
            "   10  germany        [0.55362153, 1.0]                        0.175905              0.208845               0.637352\n",
            "   11  switzerland    [0.4729466, 1.0]                         0.2789                0.292414               0.628611\n",
            "   12  brussels       [1.0, 0.5625071]                         0.313493              0.294689               0.332427\n",
            "   13  bucharest      [0.99999994, 0.46989092]                 0.450239              0.423709               0.34815\n",
            "   14  budapest       [0.99999994, 0.51703984]                 0.215203              0.239699               0.347115\n",
            "   15  cairo          [0.99999994, 0.48935854]                 0.262354              0.277823               0.242532\n",
            "   16  australia      [0.34367883, 1.0]                        0.202479              0.183715               0.387372\n",
            "   17  guinea         [0.5251478, 1.0]                         0.643715              0.71389                0.144657\n",
            "   18  copenhagen     [1.0, 0.44661847]                        0.162174              0.214494               0.210839\n",
            "   19  damascus       [1.0, 0.604804]                          0.107305              0.133331               0.22386\n",
            "   20  bangladesh     [0.45323506, 0.99999994]                 0.563048              0.562393               0.237028\n",
            "   21  dublin         [1.0, 0.43728414]                        0.0392814             0.0943912              0.057045\n",
            "   22  vietnam        [0.33919048, 1.0]                        0.351099              0.272567               0.30048\n",
            "   23  havana         [1.0, 0.4849839]                         0.329571              0.283804               0.29016\n",
            "   24  helsinki       [1.0, 0.50370324]                        0.289961              0.319788               0.260208\n",
            "   25  islamabad      [0.99999994, 0.57465935]                 0.439243              0.437042               0.207801\n",
            "   26  jakarta        [1.0, 0.4288151]                         0.366386              0.437797               0.0933146\n",
            "   27  kabul          [1.0, 0.58106303]                        0.316343              0.37099                0.212665\n",
            "   28  ukraine        [0.4809033, 1.0]                         0.457458              0.457062               0.521831\n",
            "   29  kingston       [0.99999994, 0.41654664]                 0.291586              0.241931               0.12299\n",
            "   30  peru           [0.5814075, 1.0]                         0.555149              0.520132               0.440419\n",
            "   31  lisbon         [1.0, 0.42609632]                        0.197851              0.277033               0.246217\n",
            "   33  madrid         [1.0, 0.38974234]                        0.164271              0.156943               0.297653\n",
            "   34  philippines    [0.41501483, 0.99999994]                 0.512341              0.487621               0.421939\n",
            "   35  moscow         [1.0, 0.4628907]                         0.157231              0.175006               0.277653\n",
            "   36  nairobi        [0.99999994, 0.5891436]                  0.606466              0.544422               0.263257\n",
            "   37  norway         [0.4161061, 1.0]                         0.31779               0.327994               0.535923\n",
            "   38  ottawa         [1.0, 0.5074189]                         0.359278              0.324638               0.207588\n",
            "   39  france         [0.48489687, 0.99999994]                 0.260872              0.313996               0.636397\n",
            "   40  italy          [0.44624, 1.0]                           0.235496              0.28957                1\n",
            "   41  santiago       [1.0, 0.4505333]                         0.29727               0.250458               0.23783\n",
            "   42  sofia          [1.0, 0.30064997]                        0.363592              0.361498               0.218376\n",
            "   43  stockholm      [0.99999994, 0.58504677]                 0.201825              0.220636               0.357562\n",
            "   44  taipei         [1.0, 0.65781975]                        0.459365              0.445012               0.317256\n",
            "   45  tbilisi        [1.0, 0.48339292]                        0.517062              0.579916               0.357567\n",
            "   46  iran           [0.44066894, 1.0]                        0.422406              0.412666               0.301825\n",
            "   47  tokyo          [1.0, 0.47895756]                        0.0927142             0.0985762              0.192056\n",
            "   48  austria        [0.5364096, 1.0]                         0.166846              0.254933               0.594356\n",
            "   49  poland         [0.559183, 1.0]                          0.269027              0.28901                0.535516\n",
            "   50  zagreb         [1.0, 0.4985986]                         0.42709               0.415345               0.275185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from tabulate import tabulate\n",
        "import gensim\n",
        "\n",
        "# set the paths for the necessary files\n",
        "#bats_dir = 'path/to/BATS_3.0'\n",
        "#model_file = 'path/to/reducedvector.bin'\n",
        "e01_file = 'E01 [country - capital].txt'\n",
        "\n",
        "# load the pre-trained Word2Vec model\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('reducedvector.bin', binary=True)\n",
        "\n",
        "# define a function to get the similarity between two words\n",
        "def get_similarity(word1, word2):\n",
        "    try:\n",
        "        similarity = model.similarity(word1, word2)\n",
        "        return round(similarity, 4)\n",
        "    except KeyError:\n",
        "        return None\n",
        "\n",
        "# define the target words for step 2\n",
        "target_words = ['niger', 'mali', 'italy']\n",
        "\n",
        "# initialize the table headers and data\n",
        "headers = ['Word 1', 'Word 2', 'Similarity', 'Similarity to Niger', 'Similarity to Mali', 'Similarity to Italy']\n",
        "data = []\n",
        "\n",
        "# read the lines of the E01 file and process each row\n",
        "with open(e01_file, 'r') as f:\n",
        "    for line in f:\n",
        "        # split the line into two words\n",
        "        words = line.strip().split('\\t')\n",
        "        word1, word2 = words[0], words[1]\n",
        "        \n",
        "        # choose a random target word from the row\n",
        "        target_word = random.choice(words)\n",
        "        print(\"\\nthe target word is: \", target_word)\n",
        "        \n",
        "        # get the similarity between the target word and the other word on the row\n",
        "        similarity = get_similarity(target_word, word1) or get_similarity(target_word, word2)\n",
        "        \n",
        "        # get the similarities between the target word and the three reference words\n",
        "        sim_niger = get_similarity(target_word, 'niger')\n",
        "        sim_mali = get_similarity(target_word, 'mali')\n",
        "        sim_italy = get_similarity(target_word, 'italy')\n",
        "        \n",
        "        # add the results to the table data\n",
        "        data.append([word1, word2, similarity, sim_niger, sim_mali, sim_italy])\n",
        "\n",
        "# print the table\n",
        "print(tabulate(data, headers=headers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH-sVKaR9295",
        "outputId": "c558d43a-3739-48df-9704-f14063d793fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "the target word is:  abuja\n",
            "\n",
            "the target word is:  amman\n",
            "\n",
            "the target word is:  ankara\n",
            "\n",
            "the target word is:  athens\n",
            "\n",
            "the target word is:  iraq\n",
            "\n",
            "the target word is:  bangkok\n",
            "\n",
            "the target word is:  china\n",
            "\n",
            "the target word is:  beirut\n",
            "\n",
            "the target word is:  belgrade\n",
            "\n",
            "the target word is:  berlin\n",
            "\n",
            "the target word is:  bern\n",
            "\n",
            "the target word is:  brussels\n",
            "\n",
            "the target word is:  romania\n",
            "\n",
            "the target word is:  hungary\n",
            "\n",
            "the target word is:  egypt\n",
            "\n",
            "the target word is:  australia\n",
            "\n",
            "the target word is:  conakry\n",
            "\n",
            "the target word is:  denmark\n",
            "\n",
            "the target word is:  syria\n",
            "\n",
            "the target word is:  bangladesh\n",
            "\n",
            "the target word is:  ireland\n",
            "\n",
            "the target word is:  hanoi\n",
            "\n",
            "the target word is:  cuba\n",
            "\n",
            "the target word is:  finland\n",
            "\n",
            "the target word is:  islamabad\n",
            "\n",
            "the target word is:  indonesia\n",
            "\n",
            "the target word is:  kabul\n",
            "\n",
            "the target word is:  \n",
            "\n",
            "the target word is:  kingston\n",
            "\n",
            "the target word is:  lima\n",
            "\n",
            "the target word is:  lisbon\n",
            "\n",
            "the target word is:  london\n",
            "\n",
            "the target word is:  spain\n",
            "\n",
            "the target word is:  philippines\n",
            "\n",
            "the target word is:  moscow\n",
            "\n",
            "the target word is:  nairobi\n",
            "\n",
            "the target word is:  \n",
            "\n",
            "the target word is:  ottawa\n",
            "\n",
            "the target word is:  \n",
            "\n",
            "the target word is:  \n",
            "\n",
            "the target word is:  santiago\n",
            "\n",
            "the target word is:  sofia\n",
            "\n",
            "the target word is:  stockholm\n",
            "\n",
            "the target word is:  taipei\n",
            "\n",
            "the target word is:  georgia\n",
            "\n",
            "the target word is:  tehran\n",
            "\n",
            "the target word is:  japan\n",
            "\n",
            "the target word is:  austria\n",
            "\n",
            "the target word is:  poland\n",
            "\n",
            "the target word is:  zagreb\n",
            "Word 1      Word 2                Similarity    Similarity to Niger    Similarity to Mali    Similarity to Italy\n",
            "----------  ------------------  ------------  ---------------------  --------------------  ---------------------\n",
            "abuja\n",
            "amman                                 1                      0.3763                0.3481                 0.2247\n",
            "ankara      turkey                    1                      0.289                 0.3855                 0.1875\n",
            "athens      greece                    1                      0.0683                0.1677                 0.3041\n",
            "baghdad     iraq                      0.5306                 0.2382                0.2233                 0.2368\n",
            "bangkok     thailand                  1                      0.4878                0.4616                 0.2309\n",
            "beijing     china                     0.5614                 0.3591                0.3296                 0.2944\n",
            "beirut      lebanon                   1                      0.3594                0.338                  0.2343\n",
            "belgrade    serbia                    1                      0.376                 0.3976                 0.4165\n",
            "berlin      germany                   1                     -0.0299                0.016                  0.2602\n",
            "bern                                  1                      0.3684                0.3645                 0.3287\n",
            "brussels    belgium                   1                      0.3135                0.2947                 0.3324\n",
            "bucharest   romania                   0.4699                 0.447                 0.4411                 0.6024\n",
            "budapest    hungary                   0.517                  0.2212                0.3254                 0.5952\n",
            "cairo                                 0.4894                 0.2995                0.3745                 0.4265\n",
            "canberra    australia                 0.3437                 0.2025                0.1837                 0.3874\n",
            "conakry     guinea                    1                      0.6729                0.6735                 0.3372\n",
            "copenhagen  denmark                   0.4466                 0.2747                0.3442                 0.4938\n",
            "damascus    syria                     0.6048                 0.3741                0.4006                 0.3945\n",
            "dhaka                                 0.4532                 0.563                 0.5624                 0.237\n",
            "dublin      ireland                   0.4373                 0.1101                0.1971                 0.3971\n",
            "hanoi                                 1                      0.2628                0.1567                 0.0143\n",
            "havana      cuba                      0.485                  0.4043                0.4038                 0.3442\n",
            "helsinki    finland                   0.5037                 0.3479                0.3859                 0.4306\n",
            "islamabad   pakistan                  1                      0.4392                0.437                  0.2078\n",
            "jakarta     indonesia                 0.4288                 0.5629                0.5481                 0.316\n",
            "kabul                                 1                      0.3163                0.371                  0.2127\n",
            "kiev\n",
            "kingston    jamaica                   1                      0.2916                0.2419                 0.123\n",
            "lima                                  1                      0.4905                0.4776                 0.2679\n",
            "lisbon      portugal                  1                      0.1979                0.277                  0.2462\n",
            "london      england/uk/britain        1                      0.0178                0.012                  0.1441\n",
            "madrid      spain                     0.3897                 0.2808                0.3216                 0.656\n",
            "manila      philippines               0.415                  0.5123                0.4876                 0.4219\n",
            "moscow      russia                    1                      0.1572                0.175                  0.2777\n",
            "nairobi     kenya                     1                      0.6065                0.5444                 0.2633\n",
            "oslo\n",
            "ottawa      canada                    1                      0.3593                0.3246                 0.2076\n",
            "paris\n",
            "rome\n",
            "santiago    chile                     1                      0.2973                0.2505                 0.2378\n",
            "sofia                                 1                      0.3636                0.3615                 0.2184\n",
            "stockholm   sweden                    1                      0.2018                0.2206                 0.3576\n",
            "taipei      taiwan                    1                      0.4594                0.445                  0.3173\n",
            "tbilisi     georgia                   0.4834                 0.3535                0.3638                 0.2416\n",
            "tehran      iran                      1                      0.3631                0.347                  0.1962\n",
            "tokyo                                 0.479                  0.2599                0.3259                 0.428\n",
            "vienna      austria                   0.5364                 0.1668                0.2549                 0.5944\n",
            "warsaw      poland                    0.5592                 0.269                 0.289                  0.5355\n",
            "zagreb      croatia                   1                      0.4271                0.4153                 0.2752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from gensim.models import KeyedVectors\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Load the pre-trained Word2Vec model\n",
        "#model_path = \"reducedvector.bin\"\n",
        "model = KeyedVectors.load_word2vec_format('reducedvector.bin', binary=True)\n",
        "\n",
        "# Select the E01 [country - capital].txt file\n",
        "file_path = \"E01 [country - capital].txt\"\n",
        "\n",
        "# Initialize the result table\n",
        "result_table = [[\"Word Pair\", \"Random Target Word\", \"Similarity to Target Word\", \"Similarity to Niger\", \"Similarity to Mali\", \"Similarity to Italy\"]]\n",
        "\n",
        "# Iterate through each line in the file\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            # Split the line into two words\n",
        "            word_pair = line.strip().split()\n",
        "\n",
        "            # Choose a random target word\n",
        "            target_word = random.choice(word_pair)\n",
        "\n",
        "            # Compute the similarity between the target word and the other word in the pair\n",
        "            similarity_to_pair = model.similarity(target_word, [w for w in word_pair if w != target_word][0])\n",
        "\n",
        "            # Compute the similarity between the target word and the three countries\n",
        "            similarity_to_niger = model.similarity(target_word, \"niger\")\n",
        "            similarity_to_mali = model.similarity(target_word, \"mali\")\n",
        "            similarity_to_italy = model.similarity(target_word, \"italy\")\n",
        "\n",
        "            # Append the result to the result table\n",
        "            result_table.append([word_pair, target_word, similarity_to_pair, similarity_to_niger, similarity_to_mali, similarity_to_italy])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "# Print the result table\n",
        "print(tabulate(result_table, headers=\"firstrow\", tablefmt=\"grid\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbHjpqdZ_-qa",
        "outputId": "b43c99e1-2cde-45ff-d8f7-47d367c86ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| Word Pair                 | Random Target Word   |   Similarity to Target Word |   Similarity to Niger |   Similarity to Mali |   Similarity to Italy |\n",
            "+===========================+======================+=============================+=======================+======================+=======================+\n",
            "| ['amman', 'jordan']       | amman                |                    0.324523 |             0.376343  |           0.348145   |             0.224705  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['ankara', 'turkey']      | ankara               |                    0.295108 |             0.288966  |           0.385501   |             0.187538  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['athens', 'greece']      | athens               |                    0.510705 |             0.0682994 |           0.167693   |             0.30412   |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['baghdad', 'iraq']       | iraq                 |                    0.530551 |             0.238219  |           0.223336   |             0.236815  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['bangkok', 'thailand']   | thailand             |                    0.468903 |             0.545814  |           0.50752    |             0.388154  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['beijing', 'china']      | beijing              |                    0.561427 |             0.315615  |           0.272403   |             0.119315  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['beirut', 'lebanon']     | beirut               |                    0.584435 |             0.359446  |           0.338045   |             0.2343    |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['belgrade', 'serbia']    | belgrade             |                    0.601303 |             0.376045  |           0.397636   |             0.416546  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['berlin', 'germany']     | germany              |                    0.553622 |             0.175905  |           0.208845   |             0.637352  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['bern', 'switzerland']   | switzerland          |                    0.472947 |             0.2789    |           0.292414   |             0.628611  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['brussels', 'belgium']   | brussels             |                    0.562507 |             0.313493  |           0.294689   |             0.332427  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['bucharest', 'romania']  | romania              |                    0.469891 |             0.446976  |           0.44108    |             0.602443  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['budapest', 'hungary']   | budapest             |                    0.51704  |             0.215203  |           0.239699   |             0.347115  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['cairo', 'egypt']        | egypt                |                    0.489359 |             0.299507  |           0.37445    |             0.42649   |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['canberra', 'australia'] | australia            |                    0.343679 |             0.202479  |           0.183715   |             0.387372  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['conakry', 'guinea']     | guinea               |                    0.525148 |             0.643715  |           0.71389    |             0.144657  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['copenhagen', 'denmark'] | copenhagen           |                    0.446618 |             0.162174  |           0.214494   |             0.210839  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['damascus', 'syria']     | syria                |                    0.604804 |             0.374066  |           0.400626   |             0.394456  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['dhaka', 'bangladesh']   | dhaka                |                    0.453235 |             0.374322  |           0.354951   |             0.131674  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['dublin', 'ireland']     | ireland              |                    0.437284 |             0.110095  |           0.197061   |             0.397105  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['hanoi', 'vietnam']      | hanoi                |                    0.33919  |             0.262848  |           0.156738   |             0.0142575 |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['havana', 'cuba']        | cuba                 |                    0.484984 |             0.404263  |           0.40382    |             0.344214  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['helsinki', 'finland']   | finland              |                    0.503703 |             0.347921  |           0.385884   |             0.430574  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['islamabad', 'pakistan'] | pakistan             |                    0.574659 |             0.513078  |           0.523214   |             0.357543  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['jakarta', 'indonesia']  | indonesia            |                    0.428815 |             0.562938  |           0.54807    |             0.31601   |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['kabul', 'afghanistan']  | afghanistan          |                    0.581063 |             0.357975  |           0.407937   |             0.251124  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['kiev', 'ukraine']       | ukraine              |                    0.480903 |             0.457458  |           0.457062   |             0.521831  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['kingston', 'jamaica']   | jamaica              |                    0.416547 |             0.577146  |           0.584658   |             0.313172  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['lima', 'peru']          | lima                 |                    0.581407 |             0.490463  |           0.477576   |             0.267905  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['lisbon', 'portugal']    | lisbon               |                    0.426096 |             0.197851  |           0.277033   |             0.246217  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['madrid', 'spain']       | madrid               |                    0.389742 |             0.164271  |           0.156943   |             0.297653  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['manila', 'philippines'] | philippines          |                    0.415015 |             0.512341  |           0.487621   |             0.421939  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['moscow', 'russia']      | moscow               |                    0.462891 |             0.157231  |           0.175006   |             0.277653  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['nairobi', 'kenya']      | nairobi              |                    0.589144 |             0.606466  |           0.544422   |             0.263257  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['oslo', 'norway']        | oslo                 |                    0.416106 |             0.293264  |           0.25777    |             0.380327  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['ottawa', 'canada']      | canada               |                    0.507419 |             0.182492  |           0.157154   |             0.358455  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['paris', 'france']       | paris                |                    0.484897 |             0.033879  |           0.00171089 |             0.408882  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['rome', 'italy']         | rome                 |                    0.44624  |            -0.0644078 |           0.0818814  |             0.44624   |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['santiago', 'chile']     | santiago             |                    0.450533 |             0.29727   |           0.250458   |             0.23783   |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['sofia', 'bulgaria']     | sofia                |                    0.30065  |             0.363592  |           0.361498   |             0.218376  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['stockholm', 'sweden']   | stockholm            |                    0.585047 |             0.201825  |           0.220636   |             0.357562  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['taipei', 'taiwan']      | taiwan               |                    0.65782  |             0.454594  |           0.428525   |             0.254835  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['tbilisi', 'georgia']    | tbilisi              |                    0.483393 |             0.517062  |           0.579916   |             0.357567  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['tehran', 'iran']        | tehran               |                    0.440669 |             0.363128  |           0.346985   |             0.196175  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['tokyo', 'japan']        | japan                |                    0.478958 |             0.259931  |           0.325909   |             0.428024  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['vienna', 'austria']     | austria              |                    0.53641  |             0.166846  |           0.254933   |             0.594356  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['warsaw', 'poland']      | poland               |                    0.559183 |             0.269027  |           0.28901    |             0.535516  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n",
            "| ['zagreb', 'croatia']     | croatia              |                    0.498599 |             0.510222  |           0.571782   |             0.556534  |\n",
            "+---------------------------+----------------------+-----------------------------+-----------------------+----------------------+-----------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3: \n",
        "Sentences:\n",
        "king is to throne as judge is to ___?\n",
        "giant is to dwarf as genius is to ___?\n",
        "college is to dean as jail is to ___?\n",
        "arc is to circle as line is to ___?\n",
        "French is to France as Dutch is to ___?\n",
        "man is to woman as king is to ___?\n",
        "water is to ice as liquid is to ___?\n",
        "bad is to good as sad is to ___?\n",
        "nurse is to hospital as teacher is to ___?\n",
        "usa is to pizza as japan is to ___?\n",
        "human is to house as dog is to ___?\n",
        "grass is to green as sky is to ___?\n",
        "video is to cassette as computer is to ____?\n",
        "universe is to planet as house is to ____?\n",
        "poverty is to wealth as sickness is to ___?\n",
        "a. Complete the above sentences with your own word analogies. Use the Word2Vec model to find the\n",
        "similarity measure between your pair of words. Provide your results.\n",
        "Example:\n",
        "man is to woman as king is to _queen__?\n",
        "newmodel.similarity('king', 'queen') -> 0.5685571\n",
        "b. Use the Word2Vec model to find the word analogy and corresponding similarity score. Provide\n",
        "your results.\n",
        "Example:"
      ],
      "metadata": {
        "id": "eMY57QPHu8CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from tabulate import tabulate\n",
        "\n",
        "my_model = KeyedVectors.load_word2vec_format('reducedvector.bin', binary=True)\n",
        "\n",
        "pairs = [('judge', 'bench'),\n",
        "         ('genius', 'moron'),\n",
        "         ('jail', 'warden'),\n",
        "         ('line', 'angle'),\n",
        "         ('Dutch', 'Flanders'),\n",
        "         ('king', 'queen'),\n",
        "         ('liquid', 'ice'),\n",
        "         ('sad', 'happy'),\n",
        "         ('teacher', 'school'),\n",
        "         ('japan', 'sushi'),\n",
        "         ('dog', 'kennel'),\n",
        "         ('sky', 'blue'),\n",
        "         ('computer', 'disk'),\n",
        "         ('house', 'estate'),\n",
        "         ('sickness', 'health')]\n",
        "\n",
        "results = []\n",
        "for pair in pairs:\n",
        "  try:\n",
        "    similarity = my_model.similarity(pair[0], pair[1])\n",
        "    results.append([pair[0], pair[1], similarity])\n",
        "  except KeyError:\n",
        "    continue\n",
        "\n",
        "\n",
        "print(tabulate(results, headers=['Word 1', 'Word 2', 'Similarity']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxiJEfYw9NJB",
        "outputId": "e56a43c9-1d1b-4e55-886b-fb2af214f036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word 1    Word 2      Similarity\n",
            "--------  --------  ------------\n",
            "judge     bench        0.302673\n",
            "genius    moron        0.128466\n",
            "jail      warden       0.277774\n",
            "line      angle        0.287782\n",
            "king      queen        0.568557\n",
            "liquid    ice          0.293175\n",
            "sad       happy        0.448851\n",
            "teacher   school       0.532657\n",
            "japan     sushi        0.0118663\n",
            "dog       kennel       0.28416\n",
            "sky       blue         0.44397\n",
            "computer  disk         0.358941\n",
            "house     estate       0.207919\n",
            "sickness  health       0.195276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# load the pre-trained Word2Vec model\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('reducedvector.bin', binary=True)\n",
        "\n",
        "# create a list of word pairs\n",
        "word_pairs = [('judge', 'bench'), \n",
        "              ('genius', 'moron'), \n",
        "              ('jail', 'warden'), \n",
        "              ('line', 'angle'), \n",
        "              ('Dutch', 'Netherlands'), \n",
        "              ('king', 'queen'), \n",
        "              ('liquid', 'ice'), \n",
        "              ('sad', 'happy'), \n",
        "              ('teacher', 'school'), \n",
        "              ('japan', 'sushi'), \n",
        "              ('dog', 'kennel'), \n",
        "              ('sky', 'blue'), \n",
        "              ('computer', 'disk'), \n",
        "              ('house', 'estate'), \n",
        "              ('sickness', 'health')]\n",
        "\n",
        "# create an empty Pandas dataframe\n",
        "df = pd.DataFrame(columns=['word_pair', 'similarity'])\n",
        "\n",
        "# loop through the word pairs and calculate the similarity measure\n",
        "for pair in word_pairs:\n",
        "  try:\n",
        "    similarity = model.similarity(pair[0], pair[1])\n",
        "    df = df.append({'word_pair': pair, 'similarity': similarity}, ignore_index=True)\n",
        "  except KeyError:\n",
        "    continue\n",
        "\n",
        "# print the similarity measure values in a tabular format\n",
        "print(tabulate(df, headers='keys', tablefmt='psql'))\n",
        "\n",
        "# print the dataframe\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "9wKKd2rMR7Zo",
        "outputId": "989f0373-6610-497a-b425-a6dcd0f61bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------------------+--------------+\n",
            "|    | word_pair              |   similarity |\n",
            "|----+------------------------+--------------|\n",
            "|  0 | ('judge', 'bench')     |    0.302673  |\n",
            "|  1 | ('genius', 'moron')    |    0.128466  |\n",
            "|  2 | ('jail', 'warden')     |    0.277774  |\n",
            "|  3 | ('line', 'angle')      |    0.287782  |\n",
            "|  4 | ('king', 'queen')      |    0.568557  |\n",
            "|  5 | ('liquid', 'ice')      |    0.293175  |\n",
            "|  6 | ('sad', 'happy')       |    0.448851  |\n",
            "|  7 | ('teacher', 'school')  |    0.532657  |\n",
            "|  8 | ('japan', 'sushi')     |    0.0118663 |\n",
            "|  9 | ('dog', 'kennel')      |    0.28416   |\n",
            "| 10 | ('sky', 'blue')        |    0.44397   |\n",
            "| 11 | ('computer', 'disk')   |    0.358941  |\n",
            "| 12 | ('house', 'estate')    |    0.207919  |\n",
            "| 13 | ('sickness', 'health') |    0.195276  |\n",
            "+----+------------------------+--------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             word_pair  similarity\n",
              "0       (judge, bench)    0.302673\n",
              "1      (genius, moron)    0.128466\n",
              "2       (jail, warden)    0.277774\n",
              "3        (line, angle)    0.287782\n",
              "4        (king, queen)    0.568557\n",
              "5        (liquid, ice)    0.293175\n",
              "6         (sad, happy)    0.448851\n",
              "7    (teacher, school)    0.532657\n",
              "8       (japan, sushi)    0.011866\n",
              "9        (dog, kennel)    0.284160\n",
              "10         (sky, blue)    0.443970\n",
              "11    (computer, disk)    0.358941\n",
              "12     (house, estate)    0.207919\n",
              "13  (sickness, health)    0.195276"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8333e54b-3b08-4c83-8884-f3ccd922284c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_pair</th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(judge, bench)</td>\n",
              "      <td>0.302673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(genius, moron)</td>\n",
              "      <td>0.128466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(jail, warden)</td>\n",
              "      <td>0.277774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(line, angle)</td>\n",
              "      <td>0.287782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(king, queen)</td>\n",
              "      <td>0.568557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(liquid, ice)</td>\n",
              "      <td>0.293175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>(sad, happy)</td>\n",
              "      <td>0.448851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>(teacher, school)</td>\n",
              "      <td>0.532657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(japan, sushi)</td>\n",
              "      <td>0.011866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(dog, kennel)</td>\n",
              "      <td>0.284160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>(sky, blue)</td>\n",
              "      <td>0.443970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>(computer, disk)</td>\n",
              "      <td>0.358941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>(house, estate)</td>\n",
              "      <td>0.207919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>(sickness, health)</td>\n",
              "      <td>0.195276</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8333e54b-3b08-4c83-8884-f3ccd922284c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8333e54b-3b08-4c83-8884-f3ccd922284c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8333e54b-3b08-4c83-8884-f3ccd922284c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load the pretrained Word2Vec model\n",
        "model = KeyedVectors.load_word2vec_format('reducedvector.bin', binary=True)\n",
        "\n",
        "# Define the sentences and their corresponding word analogies\n",
        "sentences = [\n",
        "    ('king', 'throne', 'judge'),\n",
        "    ('giant', 'dwarf', 'genius'),\n",
        "    ('college', 'dean', 'jail'),\n",
        "    ('arc', 'circle', 'line'),\n",
        "    ('French', 'France', 'Dutch'),\n",
        "    ('man', 'woman', 'king'),\n",
        "    ('water', 'ice', 'liquid'),\n",
        "    ('bad', 'good', 'sad'),\n",
        "    ('nurse', 'hospital', 'teacher'),\n",
        "    ('usa', 'pizza', 'japan'),\n",
        "    ('human', 'house', 'dog'),\n",
        "    ('grass', 'green', 'sky'),\n",
        "    ('video', 'cassette', 'computer'),\n",
        "    ('universe', 'planet', 'house'),\n",
        "    ('poverty', 'wealth', 'sickness')\n",
        "]\n",
        "\n",
        "# Find the word analogies and their corresponding similarity scores\n",
        "for s in sentences:\n",
        "  try:\n",
        "    analogy = model.most_similar(positive=[s[2], s[1]], negative=[s[0]])\n",
        "    print(\"{} is to {} as {} is to {}: {:.2f}\".format(s[0], s[1], s[2], analogy[0][0], analogy[0][1]))\n",
        "  except KeyError:\n",
        "    continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPrJ6Hgnki1m",
        "outputId": "a67f915a-1efb-4258-9eb4-35286198cca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "king is to throne as judge is to prosecution: 0.52\n",
            "giant is to dwarf as genius is to theorist: 0.43\n",
            "college is to dean as jail is to peress: 0.54\n",
            "arc is to circle as line is to lines: 0.43\n",
            "man is to woman as king is to queen: 0.55\n",
            "water is to ice as liquid is to solid: 0.45\n",
            "bad is to good as sad is to glory: 0.44\n",
            "nurse is to hospital as teacher is to institution: 0.48\n",
            "usa is to pizza as japan is to dishes: 0.58\n",
            "human is to house as dog is to hound: 0.42\n",
            "grass is to green as sky is to blue: 0.55\n",
            "video is to cassette as computer is to peripherals: 0.67\n",
            "universe is to planet as house is to houses: 0.43\n",
            "poverty is to wealth as sickness is to impious: 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "\n",
        "# load the pretrained Word2Vec model\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('reducedvector.bin', binary=True)\n",
        "\n",
        "# define the list of word analogies\n",
        "word_analogies = [\n",
        "    ('king', 'throne', 'judge'),\n",
        "    ('giant', 'dwarf', 'genius'),\n",
        "    ('college', 'dean', 'jail'),\n",
        "    ('arc', 'circle', 'line'),\n",
        "    ('French', 'France', 'Dutch'),\n",
        "    ('man', 'woman', 'king'),\n",
        "    ('water', 'ice', 'liquid'),\n",
        "    ('bad', 'good', 'sad'),\n",
        "    ('nurse', 'hospital', 'teacher'),\n",
        "    ('usa', 'pizza', 'japan'),\n",
        "    ('human', 'house', 'dog'),\n",
        "    ('grass', 'green', 'sky'),\n",
        "    ('video', 'cassette', 'computer'),\n",
        "    ('universe', 'planet', 'house'),\n",
        "    ('poverty', 'wealth', 'sickness')\n",
        "]\n",
        "\n",
        "# create a dataframe to store the results\n",
        "result_df = pd.DataFrame(columns=['word_analogy', 'most_similar_word', 'similarity_score'])\n",
        "\n",
        "# loop through each word analogy and find the most similar word\n",
        "for word_analogy in word_analogies:\n",
        "    # extract the three words in the analogy\n",
        "    word1, word2, word3 = word_analogy\n",
        "    \n",
        "    try:\n",
        "      # find the most similar word for the analogy\n",
        "      most_similar_word, similarity_score = model.most_similar(positive=[word3, word2], negative=[word1])[0]\n",
        "\n",
        "      # add the results to the dataframe\n",
        "      result_df = result_df.append({\n",
        "          'word_analogy': f'{word1} is to {word2} as {word3} is to ___?',\n",
        "          'most_similar_word': most_similar_word,\n",
        "          'similarity_score': similarity_score\n",
        "      }, ignore_index=True)\n",
        "    except KeyError:\n",
        "      continue\n",
        "    \n",
        "    \n",
        "\n",
        "# print the results\n",
        "result_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "a_eNSE_GzWxM",
        "outputId": "9c0f9577-1d7e-4f6a-9a58-5cde5564ffe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   word_analogy most_similar_word  \\\n",
              "0         king is to throne as judge is to ___?       prosecution   \n",
              "1        giant is to dwarf as genius is to ___?          theorist   \n",
              "2         college is to dean as jail is to ___?            peress   \n",
              "3           arc is to circle as line is to ___?             lines   \n",
              "4            man is to woman as king is to ___?             queen   \n",
              "5          water is to ice as liquid is to ___?             solid   \n",
              "6              bad is to good as sad is to ___?             glory   \n",
              "7    nurse is to hospital as teacher is to ___?       institution   \n",
              "8           usa is to pizza as japan is to ___?            dishes   \n",
              "9           human is to house as dog is to ___?             hound   \n",
              "10          grass is to green as sky is to ___?              blue   \n",
              "11  video is to cassette as computer is to ___?       peripherals   \n",
              "12    universe is to planet as house is to ___?            houses   \n",
              "13  poverty is to wealth as sickness is to ___?           impious   \n",
              "\n",
              "    similarity_score  \n",
              "0           0.518646  \n",
              "1           0.428089  \n",
              "2           0.544443  \n",
              "3           0.428753  \n",
              "4           0.553245  \n",
              "5           0.450004  \n",
              "6           0.440382  \n",
              "7           0.482898  \n",
              "8           0.576351  \n",
              "9           0.423166  \n",
              "10          0.547864  \n",
              "11          0.665451  \n",
              "12          0.426470  \n",
              "13          0.496061  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aba69f51-c8c1-4b57-96b1-d23fd8d5c151\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_analogy</th>\n",
              "      <th>most_similar_word</th>\n",
              "      <th>similarity_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>king is to throne as judge is to ___?</td>\n",
              "      <td>prosecution</td>\n",
              "      <td>0.518646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>giant is to dwarf as genius is to ___?</td>\n",
              "      <td>theorist</td>\n",
              "      <td>0.428089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>college is to dean as jail is to ___?</td>\n",
              "      <td>peress</td>\n",
              "      <td>0.544443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>arc is to circle as line is to ___?</td>\n",
              "      <td>lines</td>\n",
              "      <td>0.428753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>man is to woman as king is to ___?</td>\n",
              "      <td>queen</td>\n",
              "      <td>0.553245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>water is to ice as liquid is to ___?</td>\n",
              "      <td>solid</td>\n",
              "      <td>0.450004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>bad is to good as sad is to ___?</td>\n",
              "      <td>glory</td>\n",
              "      <td>0.440382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>nurse is to hospital as teacher is to ___?</td>\n",
              "      <td>institution</td>\n",
              "      <td>0.482898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>usa is to pizza as japan is to ___?</td>\n",
              "      <td>dishes</td>\n",
              "      <td>0.576351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>human is to house as dog is to ___?</td>\n",
              "      <td>hound</td>\n",
              "      <td>0.423166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>grass is to green as sky is to ___?</td>\n",
              "      <td>blue</td>\n",
              "      <td>0.547864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>video is to cassette as computer is to ___?</td>\n",
              "      <td>peripherals</td>\n",
              "      <td>0.665451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>universe is to planet as house is to ___?</td>\n",
              "      <td>houses</td>\n",
              "      <td>0.426470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>poverty is to wealth as sickness is to ___?</td>\n",
              "      <td>impious</td>\n",
              "      <td>0.496061</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aba69f51-c8c1-4b57-96b1-d23fd8d5c151')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aba69f51-c8c1-4b57-96b1-d23fd8d5c151 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aba69f51-c8c1-4b57-96b1-d23fd8d5c151');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N17VvGzZAg6",
        "outputId": "8eda0289-1bef-42ef-ba97-cd1b7a65817d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "word_pair      object\n",
              "similarity    float32\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAbtYFwDZbKx",
        "outputId": "e06b0e66-4ebc-437d-b62d-0d8ce4524925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "word_analogy          object\n",
              "most_similar_word     object\n",
              "similarity_score     float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# convert the float32 column to float64 using astype\n",
        "df['similarity'] = df['similarity'].astype('float64')\n",
        "\n",
        "# compute the correlation using the corr method\n",
        "correlation = df['similarity'].corr(result_df['similarity_score'])\n",
        "\n",
        "# display the results using tabulate\n",
        "print(tabulate([[\"Correlation\", correlation]], headers=[\"Metric\", \"Value\"], tablefmt=\"orgtbl\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAI-BHDAXQxm",
        "outputId": "b8c9dc76-afb1-40f9-c3bc-4d9785e9c88c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Metric      |    Value |\n",
            "|-------------+----------|\n",
            "| Correlation | 0.119729 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Task Set #2: For this part of the assignment, we will work with the UTK dataset (UTKface_cropped.tar.gz) available on Canvas and based on the original UTKFace dataset (https://susanqq.github.io/UTKFace/)**"
      ],
      "metadata": {
        "id": "axlVt4ddfqjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1: \n",
        "Each image in the dataset has a unique value representing age, gender, and race based on the\n",
        "following legend:\n",
        "â— age: indicates the age of the person in the picture and can range from 0 to 116.\n",
        "â— gender: indicates the gender of the person and is either 0 (male) or 1 (female).\n",
        "â— race: indicates the race of the person and can from 0 to 4, denoting White, Black, Asian, Indian,\n",
        "and Others (like Hispanic, Latino, Middle Eastern).\n",
        "Complete and answer the following:\n",
        "â€¢ Compute and document the frequency of images associated with each subgroup for age\n",
        "(subdivide based on - (0-20), (21,40), (41,60), (61,80), (81, 116)), gender (0,1), and race (0 to 4)."
      ],
      "metadata": {
        "id": "9ewC2pfNkMFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Unzip and open the crop_part1.zip zipped folder\n",
        "with zipfile.ZipFile('crop_part1.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# Step 2: Create age bins of 0-20, 21-40, 41-60, 61-80 and 81-116\n",
        "age_bins = {'0-20': range(0, 21), '21-40': range(21, 41), '41-60': range(41, 61), \n",
        "            '61-80': range(61, 81), '81-116': range(81, 117)}\n",
        "\n",
        "# Step 3: Create gender bins or classifications. 0 represents male while 1 represents female\n",
        "gender_bins = {0: 'male', 1: 'female'}\n",
        "\n",
        "# Step 4: Create race bins of 0 to 4. 0 denotes White, 1 denotes Black, 2 denotes Asian, 3 denotes Indian, 4 denotes Others\n",
        "race_bins = {0: 'White', 1: 'Black', 2: 'Asian', 3: 'Indian', 4: 'Others'}\n",
        "\n",
        "# Step 5: Loop through all the file names and group by age and gender\n",
        "age_gender_counts = defaultdict(lambda: defaultdict(int))\n",
        "for file_name in os.listdir('crop_part1'):\n",
        "    age = int(file_name.split('_')[0])\n",
        "    gender = int(file_name.split('_')[2])\n",
        "    for age_bin_name, age_bin_range in age_bins.items():\n",
        "        if age in age_bin_range:\n",
        "          try:\n",
        "            gender_bin_name = gender_bins[gender]\n",
        "            age_gender_counts[age_bin_name][gender_bin_name] += 1\n",
        "            break\n",
        "          except KeyError:\n",
        "            continue\n",
        "\n",
        "# Step 6: Loop through all the file names and group by age and race\n",
        "age_race_counts = defaultdict(lambda: defaultdict(int))\n",
        "for file_name in os.listdir('crop_part1'):\n",
        "    age = int(file_name.split('_')[0])\n",
        "    race = int(file_name.split('_')[3])\n",
        "    for age_bin_name, age_bin_range in age_bins.items():\n",
        "        if age in age_bin_range:\n",
        "          try:\n",
        "            race_bin_name = race_bins[race]\n",
        "            age_race_counts[age_bin_name][race_bin_name] += 1\n",
        "            break\n",
        "          except KeyError:\n",
        "            continue\n",
        "\n",
        "# Step 7: Create dataframes\n",
        "df_age_gender_counts = pd.DataFrame(age_gender_counts).T\n",
        "df_age_race_counts = pd.DataFrame(age_race_counts).T\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "TyoI9haZm04y",
        "outputId": "e913652e-d68b-4355-9567-4bdf88ec01a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-260bf946a6b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'crop_part1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mage_bin_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_bin_range\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mage_bins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mage_bin_range\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '20161219222236983.jpg.chip.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Display the age/gender dataframe\n",
        "print(\"Age and Gender Counts:\")\n",
        "df_age_gender_counts"
      ],
      "metadata": {
        "id": "Fghzjb9Rm5nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Display the age/race dataframe\n",
        "print(\"\\nAge and Race Counts:\")\n",
        "df_age_race_counts"
      ],
      "metadata": {
        "id": "TiyjLDN5m-w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "# set up age, gender, and race bins\n",
        "age_bins = [0, 20, 40, 60, 80, 116]\n",
        "gender_bins = [0, 1]\n",
        "race_bins = [0, 1, 2, 3, 4]\n",
        "\n",
        "# create dictionary to hold age and gender counts\n",
        "age_gender_counts = {}\n",
        "for i in range(len(age_bins)-1):\n",
        "    age_gender_counts[i] = {'M': 0, 'F': 0}\n",
        "\n",
        "# create dictionary to hold age and race counts\n",
        "age_race_counts = {}\n",
        "for i in range(len(age_bins)-1):\n",
        "    age_race_counts[i] = {'W': 0, 'B': 0, 'A': 0, 'I': 0, 'O': 0}\n",
        "\n",
        "# unzip the dataset\n",
        "with zipfile.ZipFile('crop_part1.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('crop_part1')\n",
        "\n",
        "# loop through all file names and count age, gender, and race\n",
        "for file_name in os.listdir('crop_part1'):\n",
        "    if file_name.endswith('.jpg'):\n",
        "        age = int(file_name.split('_')[0])\n",
        "        gender = int(file_name.split('_')[1])\n",
        "        race = int(file_name.split('_')[2])\n",
        "\n",
        "        # count age and gender\n",
        "        for i in range(len(age_bins)-1):\n",
        "            if age >= age_bins[i] and age < age_bins[i+1]:\n",
        "                if gender == 0:\n",
        "                    age_gender_counts[i]['M'] += 1\n",
        "                elif gender == 1:\n",
        "                    age_gender_counts[i]['F'] += 1\n",
        "\n",
        "        # count age and race\n",
        "        for i in range(len(age_bins)-1):\n",
        "            if age >= age_bins[i] and age < age_bins[i+1]:\n",
        "                if race == 0:\n",
        "                    age_race_counts[i]['W'] += 1\n",
        "                elif race == 1:\n",
        "                    age_race_counts[i]['B'] += 1\n",
        "                elif race == 2:\n",
        "                    age_race_counts[i]['A'] += 1\n",
        "                elif race == 3:\n",
        "                    age_race_counts[i]['I'] += 1\n",
        "                elif race == 4:\n",
        "                    age_race_counts[i]['O'] += 1\n",
        "\n",
        "# create dataframes for age and gender counts\n",
        "age_gender_df = pd.DataFrame(age_gender_counts).T\n",
        "age_gender_df.index = ['0-20', '21-40', '41-60', '61-80', '81-116']\n",
        "age_gender_df.columns = ['Male', 'Female']\n",
        "\n",
        "# create dataframes for age and race counts\n",
        "age_race_df = pd.DataFrame(age_race_counts).T\n",
        "age_race_df.index = ['0-20', '21-40', '41-60', '61-80', '81-116']\n",
        "age_race_df.columns = ['White', 'Black', 'Asian', 'Indian', 'Others']\n",
        "\n",
        "# display dataframes\n",
        "print('Age and Gender Counts:\\n', age_gender_df)\n",
        "print('\\nAge and Race Counts:\\n', age_race_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vypuwx6AxO4",
        "outputId": "839b2852-5594-4643-ef25-eea34a6a79bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age and Gender Counts:\n",
            "         Male  Female\n",
            "0-20    1921    2243\n",
            "21-40    871    1680\n",
            "41-60    924     689\n",
            "61-80    509     524\n",
            "81-116   147     270\n",
            "\n",
            "Age and Race Counts:\n",
            "         White  Black  Asian  Indian  Others\n",
            "0-20     1888    159   1002     591     524\n",
            "21-40    1021     98    357     606     469\n",
            "41-60    1187     71     92     166      97\n",
            "61-80     853     57     47      66      10\n",
            "81-116    316     20     55      23       3\n"
          ]
        }
      ]
    }
  ]
}